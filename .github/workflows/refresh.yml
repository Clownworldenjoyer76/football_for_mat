# .github/workflows/refresh.yml
name: Weekly Data Refresh

on:
  schedule:
    # Every Tuesday at 17:00 UTC (1:00 PM ET)
    - cron: "0 17 * * 2"
  workflow_dispatch:

jobs:
  refresh-data:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # ---- R step to fetch RECENT depth charts via nflreadr ----
      - name: Set up R
        uses: r-lib/actions/setup-r@v2
        with:
          r-version: '4.3.3'

      - name: Install R packages
        run: |
          Rscript -e 'install.packages(c("remotes","jsonlite"), repos="https://cloud.r-project.org")'
          Rscript -e 'remotes::install_cran(c("nflreadr","arrow"))'

      - name: Fetch depth charts (write depth_charts.csv.gz for Python to canonicalize)
        run: |
          Rscript - <<'RSCRIPT'
          dir.create("data/raw/nflverse", recursive = TRUE, showWarnings = FALSE)
          suppressPackageStartupMessages({
            library(nflreadr)
            library(jsonlite)
          })
          dc <- nflreadr::load_depth_charts()  # recent/rolling data
          if (nrow(dc) == 0) stop("No depth chart rows returned")
          # write gzip CSV
          out_csv <- file.path("data/raw/nflverse","depth_charts.csv.gz")
          con <- gzfile(out_csv, open = "wt")
          on.exit(close(con), add = TRUE)
          write.table(dc, con, sep = ",", row.names = FALSE, col.names = TRUE, qmethod = "double")
          # optional parquet (Python will also write its own)
          try({
            suppressPackageStartupMessages(library(arrow))
            out_parquet <- file.path("data/raw/nflverse","depth_charts.parquet")
            arrow::write_parquet(dc, out_parquet)
          }, silent = TRUE)
          cat("Wrote:", out_csv, "\n")
          RSCRIPT

      # ---- Python steps (your existing flow) ----
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pyarrow fastparquet || true

      - name: Pull core NFL datasets (weekly/pbp/rosters/depth)
        env:
          NFLVERSE_WEEKLY_URL_TPL: "https://github.com/nflverse/nflverse-data/releases/download/weekly/weekly_{year}.csv.gz"
          NFLVERSE_ROSTERS_URL_TPL: "https://github.com/nflverse/nflverse-data/releases/download/rosters/rosters_{year}.csv.gz"
          # depth is provided by the R step above; still allow a URL if you add one later
          NFLVERSE_DEPTH_URL_TPL: ""
        run: |
          python scripts/01_pull_nflverse.py --start 2019 --dl-weekly --dl-rosters --dl-depth

      - name: Commit & push updated data
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -f data/raw/nflverse
          git commit -m "chore(data): weekly refresh (nflverse + depth charts) [skip ci]" || echo "No changes to commit"
          git push
