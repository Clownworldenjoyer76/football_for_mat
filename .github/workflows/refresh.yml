name: Weekly Data Refresh

on:
  schedule:
    - cron: "0 17 * * 2"   # 17:00 UTC = 12:00 PM ET (Tuesday)
  workflow_dispatch:
    inputs:
      season:
        description: "Season context (e.g., 2025)"
        required: false
        default: "2025"

permissions:
  contents: write

jobs:
  refresh-data:
    runs-on: ubuntu-latest
    env:
      SEASON: ${{ inputs.season || '2025' }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # ---------- R: depth charts via nflreadr ----------
      - name: Set up R
        uses: r-lib/actions/setup-r@v2
        with:
          r-version: '4.3.3'

      - name: Install R deps (nflreadr)
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          packages: |
            any::nflreadr

      - name: Fetch depth charts (CSV then gzip)
        run: |
          Rscript - <<'RSCRIPT'
          dir.create("data/raw/nflverse", recursive = TRUE, showWarnings = FALSE)
          suppressPackageStartupMessages(library(nflreadr))
          dc <- nflreadr::load_depth_charts()
          if (nrow(dc) == 0) stop("No depth chart rows returned")
          out_csv <- file.path("data/raw/nflverse","depth_charts.csv")
          write.csv(dc, out_csv, row.names = FALSE)
          cat("Wrote:", out_csv, "\n")
          RSCRIPT
          gzip -f data/raw/nflverse/depth_charts.csv

      # ---------- Python pulls (no external weekly URLs; your script still handles pbp/rosters/depth) ----------
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install pandas requests pyarrow fastparquet lxml || true

      - name: Pull & canonicalize nflverse datasets (pbp/rosters/depth)
        run: |
          set -e
          # Your script resolves latest PBP and canonicalizes rosters/depth to *_latest.* files
          python scripts/01_pull_nflverse.py --dl-rosters --dl-depth

          echo "---- Canonical files present in data/raw/nflverse ----"
          ls -lh data/raw/nflverse || true

      # ---------- NEW: Fetch WEEKLY via nflreadr for the requested season ----------
      - name: Fetch weekly (player-week) via nflreadr for season
        run: |
          set -e
          Rscript - <<'RSCRIPT'
          dir.create("data/raw/nflverse", recursive = TRUE, showWarnings = FALSE)
          suppressPackageStartupMessages(library(nflreadr))
          seas <- as.integer(Sys.getenv("SEASON", "2025"))
          # nflreadr: weekly player stats for the given season
          wk <- nflreadr::load_player_stats(seasons = seas)
          if (nrow(wk) == 0) stop(paste("No weekly rows returned for season", seas))
          out_csv <- file.path("data/raw/nflverse","weekly_latest.csv")
          write.csv(wk, out_csv, row.names = FALSE)
          cat("Wrote weekly to:", out_csv, "rows:", nrow(wk), "\n")
          RSCRIPT
          gzip -f data/raw/nflverse/weekly_latest.csv
          ls -lh data/raw/nflverse/weekly_latest.csv.gz

      # ---------- Build data/weekly/latest.csv from canonical weekly_latest.csv.gz ----------
      - name: Build data/weekly/latest.csv from canonical weekly_latest.csv.gz
        run: |
          set -e
          mkdir -p data/weekly
          SRC="data/raw/nflverse/weekly_latest.csv.gz"
          if [ ! -f "$SRC" ]; then
            echo "ERROR: $SRC not found after weekly fetch."
            ls -lh data/raw/nflverse || true
            exit 1
          fi
          zcat "$SRC" > data/weekly/latest.csv
          echo "âœ“ Wrote data/weekly/latest.csv from $SRC"
          head -20 data/weekly/latest.csv || true

      # ---------- Verify the weekly file actually advanced ----------
      - name: Verify season/week coverage in weekly/latest.csv
        run: |
          set -e
          python - <<'PY'
          import pandas as pd, os, sys
          want = int(os.getenv("SEASON", "2025"))
          df = pd.read_csv("data/weekly/latest.csv")
          for col in ("season","week"):
              if col not in df.columns:
                  print(f"ERROR: weekly/latest.csv missing '{col}' column", file=sys.stderr)
                  sys.exit(1)
          mn = int(df["season"].min()); mx = int(df["season"].max())
          by = df.groupby("season")["week"].max().to_dict()
          print("min season:", mn, "max season:", mx, "max week per season:", {int(k): int(v) for k,v in by.items()})
          if mx != want:
              print(f"ERROR: weekly/latest.csv max(season)={mx} != requested {want}", file=sys.stderr)
              sys.exit(1)
          PY

      # ---------- Commit & push ----------
      - name: Commit & push updated data
        run: |
          set -e
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add -f data/raw/nflverse        2>/dev/null || true
          git add -f data/raw/weather         2>/dev/null || true
          git add -f data/raw/injuries        2>/dev/null || true
          git add -f data/odds                2>/dev/null || true
          git add -f data/weekly/latest.csv   2>/dev/null || true

          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "chore(data): weekly refresh (season ${SEASON}) via nflreadr -> weekly_latest.csv.gz -> latest.csv [skip ci]"
          git fetch origin
          git pull --rebase origin main || true
          git push
